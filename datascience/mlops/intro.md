# Introduction 

## Trends in Machine and Deep Learning 

What has transpired is nothing short of remarkable: over the past four years, Natural Language Processing (NLP) has experienced _generations_ of Transformer-based models. Simultaneous with this uptake of the attention-based paradigm for leading-edge NLP, has been an exponential growth in model complexity.  That this growth has been literally exponential is made clear through the semi-log plot of the figure below.  Capturing data over just the past four years or so, model complexity has increased by three orders of magnitude or a factor of roughly 1,000. To underscore this point: ‘latest challenger’ MT-NLP is more than 5,000 times more complex than ELMo.  

Given that Transformer-based approaches are really a recent introduction, and that they are already responsible for impressive advances in NLP, a prudent conclusion is that the trend evident in the figure is likely to persist into the foreseeable future. Already the case from their introduction in 2017, compute-intensive training of transformer-based models exceeds the capacity and capability of most IT infrastructures for Deep Learning in practical terms.

Moreover, complex models retain their complexity even when a pre-trained version is employed for inferencing purposes. Thus the ‘burden’ of current-generation Transformer-based models is even felt when making inferences. Whereas this inferencing-based concern may not be as potentially debilitating as that relating to training, it is unlikely to be inconsequential. Such concerns may, for example, limit deployment options for inferencing at the edge.

The disruptive impact of Transformer-based models is not restricted to NLP. Application of this attention-centric paradigm for architecting Deep Learning models is already being applied to use cases unrelated to natural language – e.g., in support of Computer Vision. Transformer-based models underscore the impact of model complexity and extremely large volumes of training data on IT infrastructure – an impact, of course, that applies to use cases of all kinds. With the pressing need to support increasingly autonomous AI use cases (e.g., self-driving vehicles), whose success depends upon input from a plethora of multi-sensory devices, comparable and escalating demands originate from other areas of Deep Learning.  

From NLP to other use cases for Deep Learning, there exists a clear and present situation of exponentially escalating concern; there is literally no option but to refactor the supporting IT infrastructure. In other words, the need for an AI-first infrastructure for Deep Learning is unquestionable. Less obvious, perhaps, is the requirement for a broad and deep toolchain to ensure that any AI-first infrastructure for Deep Learning can be leveraged in practice - and hence the need for MLOps. To accommodate Transformers alongside existing and emerging use cases for Deep Learning, a tightly integrated ecosystem comprised of both software and hardware is required. 