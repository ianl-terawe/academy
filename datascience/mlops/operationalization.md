# Operationalizing Models 

## Overview via Azure Machine Learning 

Jupyter Notebooks and IDEs allow developers of Deep Learning applications to run their code. When crafting prototypes of algorithmic implementations to entire models, this simplest of approaches can be effective for training, validation, and inferencing purposes. However, the need to properly operationalize the lifecycle of Deep Learning models can escalate rapidly – even before production implementations are established. 

![AML Overview](https://raw.githubusercontent.com/ianl-terawe/academy/main/datascience/mlops/media/automl.png "AML Overview")

Working progressively from left-to-right in the above figure, the essentials of the service are:

- Data - Their innate ability to be driven by extremely large volumes of data is what differentiates Deep Learning models. As the figure illustrates, such models may need to leverage data that resides in relational databases to file stores, or remotely stored data that can only be retrieved via API calls. Thus, a critical enabler of the Azure ML service is that of the underlying data platform available on Azure. The introduction of a data lake (e.g., via Azure Data Lake ), for example, allows data from multiple sources to be aggregated into a single storage namespace. Even though the data can retain its original format (e.g., from unstructured to semi-structured to structured), by localizing it in this fashion via a data lake, models can be more easily trained and validated. The ability to implement scalable solutions for production use is another benefit of a modern data platform that accounts for all the attributes of Big Data. 

- Build and Train Models – From support for traditional Jupyter Notebooks and IDEs, to the no/low-code option of Azure Machine Learning Designer, and finally to Azure Automated Machine Learning meta-learning model development, Azure Machine Learning provides options for developers. Not only does this allow developers to make use of the option best suited to their skill level (from no-code novice to seasoned professional), it also permits alignment with the developer’s intent.  Azure Machine Learning clearly broadens and deepens the possibilities for building and training models that serve initially as prototypes and ultimately in production. Additionally, Azure ML allows developers to again leverage ONNX – this time for runtime optimizations as opposed to interoperability between frameworks for Machine and Deep Learning. 

- Train Models – Model training is the most compute intensive aspect of the entire Machine Learning lifecycle. As models increase in complexity, so do the demands placed upon the compute infrastructure required to train them.  As Azure ML is fully integrated with the GPU computing capabilities available on Azure, model training can be scaled up to take advantage of one or more of these accelerators available from an isolated virtual machine. As complexity and/or data volumes increase, Azure ML allows model training to be scaled up and out across a distributed network of VMs.

- Validate – Once they have been trained, but prior to their use in production, models require validation.  Because containerization is a best practice for deploying and scaling models that will be used in production (see below), it makes sense to introduce this means for packaging the model and all its runtime requirements at this stage. Once available, the containerized implementation of the model can be validated on the data that was retained for this purpose. As noted in the schematic, use of Azure Container Instance is ideally suited to the needs of this validation phase. Moreover, uptake of containerization at this stage aligns well with DevOps principles that promote a proactive stance towards maintaining models (see below). 

- Deploy Models – Open-source Kubernetes is the de facto standard for orchestrating containers such as the validated pre-trained model envisaged here. The Azure Kubernetes Service (AKS) is an implementation of this ‘middleware’ available on Azure. Through its innate ability to replicate containers, AKS ensures that pre-trained can be scaled in response to demand for inferencing purposes.  From the cloud to the edge, uptake of AKS ensures that deployments are secure in addition to being scalable. 

- Monitor – Fundamental to operationalizing models is monitoring. From metrics to logs, Azure ML provides monitoring capabilities that can quantify aspects of the development process, through to production-grade deployments. Because monitoring establishes an unambiguous means for tracking their performance in production, the underpinnings of a proactive approach for maintaining models is established. 

- CI/CD – When it comes to Machine Learning applications, change is inevitable. For example, the need to incorporate more-diverse sets of data may trigger the need for retraining a Deep Learning application. As requirements evolve, so will the code, algorithms, methods, etc., needed to realize the application. Making use of code repositories (e.g., via GitHub) is therefore a best practice – a practice whose value escalates rapidly when a development team is involved (as opposed to an isolated individual). Use of GitHub is a prerequisite for implementing DevOps principles. For example, when changes to code are made, a cycle of Continuous Integration and Continuous Deployment (CI/CD) can be enabled through use of Azure DevOps. As illustrated in the figure, this Azure DevOps integration can trigger the retraining of a model. And assuming the retrained model can be properly validated, subsequent deployment and monitoring follows to complete this valuable DevOps feedback loop. 

## Azure Machine Learning for MLOps

Taken collectively then, Azure ML operationalizes the development, use, and ongoing maintenance of Machine and Deep Learning applications – in other words, it offers lifecycle management from end-to-end. Although it has not been drawn out here to any degree, Azure ML can be applied to multiple applications. For example, using its innate capability to compose applications into interdependent workflows, software pipelines are a core competence of Azure ML. 

From model lifecycle management to pipelined applications, Azure ML delivers state-of-the-art capabilities for **Machine Learning Operations (MLOps)** in the setting of a public cloud. Even though it possesses enterprise-grade capabilities that may only become relevant as production applications are scaled in response to demand, it is important to note that there is absolutely no downside to adopting Azure ML at outset. In other words, from the earliest stages of model prototyping, it is a best practice to adopt Azure ML. As introduced above, Azure ML offers possibilities for better informing and rapidly advancing models during the earliest stages of development. Moreover, as needs inevitably escalate from modest to increasingly demanding, Azure ML can offer considerable value – end-to-end value in the lifecycle management of pipelined applications.

Deep Learning models can rapidly become ‘involved’ – even though algorithms implemented at each layer can be readily deconstructed. Consequently, it typically becomes a challenge to provide the rationale for inferenced outcomes that originate within Deep Learning models. Thus, the explainability (a dimension of Responsible AI) of Deep Learning models is an increasingly pressing requirement. Responsible AI is a necessarily diverse and interdisciplinary topic that receives significant coverage via Azure ML. 